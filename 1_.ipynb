{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Apache Spark, the SparkContext is the entry point for all Spark functionality. It represents the connection to a Spark cluster and allows you to interact with it. Itâ€™s responsible for managing the job execution, scheduling, and resource allocation for tasks.\n",
    "\n",
    "- Here are a few key points about SparkContext:\n",
    "\n",
    "- Initialization: A SparkContext is created when you run a Spark job. For instance, in PySpark, you'd create it with something like sc = SparkContext().\n",
    "\n",
    "- Cluster Connection: SparkContext connects to the Spark cluster, whether it's a local machine or a distributed cluster in the cloud (like AWS, Hadoop, etc.).\n",
    "\n",
    "- Job Execution: It coordinates the execution of Spark jobs. A job in Spark is divided into smaller tasks and distributed across the cluster.\n",
    "\n",
    "- Resource Management: It manages the resources (CPU, memory) allocated to the tasks.\n",
    "\n",
    "- RDD Creation: SparkContext is responsible for creating Resilient Distributed Datasets (RDDs), which are the fundamental data structure in Spark.\n",
    "\n",
    "- Job Submission: It submits jobs to the cluster and handles scheduling.\n",
    "\n",
    "To summarize, the SparkContext is critical for managing and executing tasks in Apache Spark. When you're using Spark for distributed data processing, the SparkContext acts as the entry point for interacting with the cluster and for launching your operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- when sparkContext interacts with the clusterManager(RM,YARN in Hadoop)\n",
    "- we Will have NodeManager running on multiple machines , each machine have RAM and CPU core allocated for node manager.\n",
    "on same machine we also have DataNode running .\n",
    "- when ever application want to process data,application contact cluster manager via spark context  ,(CM)it makes request to node manager asking for containers. \n",
    "- once container is approved ResourceManager(or CM) will start an extra code called AppMaster ,which will run in one of the containers,and it (AM) will manage the other containers which is allocated by node managers to run Tasks.within this container we will have an executer process, and this executer process will run tasks ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkClusterManagers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- spark can work in standalone mode(without hadoop)\n",
    "   - applications submitted to spark standalone mode will run FIFO order\n",
    "   - spark standalone cluster ,where we can have multiple nodes,on one of the node we can have master node and on the others we can have spark workers processes running.\n",
    "- spark with apache mesos\n",
    "  -  apache mesos is an open source project to manage computer clusters,and can also run hadoop applications.\n",
    "- spark with hadoop yarn\n",
    "  - apache yarn is the cluster resource manager of hadoop 2.spark can run on yarn.\n",
    "- spark with kubernetes\n",
    "  - kubernetes is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications. Spark can run on kubernetes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
